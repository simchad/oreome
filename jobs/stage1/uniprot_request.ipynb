{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r =requests.get('https://rest.uniprot.org/idmapping/run')\n",
    "\n",
    "print(r.status_code)\n",
    "print(r.url)\n",
    "print(r.encoding)\n",
    "print(r.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying in 3s\n",
      "https://rest.uniprot.org/idmapping/uniprotkb/results/0ccfb6e2755a4699f91886212718de3cbc938523\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import zlib\n",
    "from xml.etree import ElementTree\n",
    "from urllib.parse import urlparse, parse_qs, urlencode\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "\n",
    "POLLING_INTERVAL = 3\n",
    "API_URL = \"https://rest.uniprot.org\"\n",
    "\n",
    "\n",
    "retries = Retry(total=5, backoff_factor=0.25, status_forcelist=[500, 502, 503, 504])\n",
    "session = requests.Session()\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "\n",
    "def check_response(response):\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "    except requests.HTTPError:\n",
    "        print(response.json())\n",
    "        raise\n",
    "\n",
    "\n",
    "def submit_id_mapping(from_db, to_db, ids):\n",
    "    request = requests.post(\n",
    "        f\"{API_URL}/idmapping/run\",\n",
    "        data={\"from\": from_db, \"to\": to_db, \"ids\": \",\".join(ids)},\n",
    "    )\n",
    "    check_response(request)\n",
    "    return request.json()[\"jobId\"]\n",
    "\n",
    "\n",
    "def get_next_link(headers):\n",
    "    re_next_link = re.compile(r'<(.+)>; rel=\"next\"')\n",
    "    if \"Link\" in headers:\n",
    "        match = re_next_link.match(headers[\"Link\"])\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "\n",
    "def check_id_mapping_results_ready(job_id):\n",
    "    while True:\n",
    "        request = session.get(f\"{API_URL}/idmapping/status/{job_id}\")\n",
    "        check_response(request)\n",
    "        j = request.json()\n",
    "        if \"jobStatus\" in j:\n",
    "            if j[\"jobStatus\"] == \"RUNNING\":\n",
    "                print(f\"Retrying in {POLLING_INTERVAL}s\")\n",
    "                time.sleep(POLLING_INTERVAL)\n",
    "            else:\n",
    "                raise Exception(j[\"jobStatus\"])\n",
    "        else:\n",
    "            return bool(j[\"results\"] or j[\"failedIds\"])\n",
    "\n",
    "\n",
    "def get_batch(batch_response, file_format, compressed):\n",
    "    batch_url = get_next_link(batch_response.headers)\n",
    "    while batch_url:\n",
    "        batch_response = session.get(batch_url)\n",
    "        batch_response.raise_for_status()\n",
    "        yield decode_results(batch_response, file_format, compressed)\n",
    "        batch_url = get_next_link(batch_response.headers)\n",
    "\n",
    "\n",
    "def combine_batches(all_results, batch_results, file_format):\n",
    "    if file_format == \"json\":\n",
    "        for key in (\"results\", \"failedIds\"):\n",
    "            if key in batch_results and batch_results[key]:\n",
    "                all_results[key] += batch_results[key]\n",
    "    elif file_format == \"tsv\":\n",
    "        return all_results + batch_results[1:]\n",
    "    else:\n",
    "        return all_results + batch_results\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def get_id_mapping_results_link(job_id):\n",
    "    url = f\"{API_URL}/idmapping/details/{job_id}\"\n",
    "    request = session.get(url)\n",
    "    check_response(request)\n",
    "    return request.json()[\"redirectURL\"]\n",
    "\n",
    "\n",
    "def decode_results(response, file_format, compressed):\n",
    "    if compressed:\n",
    "        decompressed = zlib.decompress(response.content, 16 + zlib.MAX_WBITS)\n",
    "        if file_format == \"json\":\n",
    "            j = json.loads(decompressed.decode(\"utf-8\"))\n",
    "            return j\n",
    "        elif file_format == \"tsv\":\n",
    "            return [line for line in decompressed.decode(\"utf-8\").split(\"\\n\") if line]\n",
    "        elif file_format == \"xlsx\":\n",
    "            return [decompressed]\n",
    "        elif file_format == \"xml\":\n",
    "            return [decompressed.decode(\"utf-8\")]\n",
    "        else:\n",
    "            return decompressed.decode(\"utf-8\")\n",
    "    elif file_format == \"json\":\n",
    "        return response.json()\n",
    "    elif file_format == \"tsv\":\n",
    "        return [line for line in response.text.split(\"\\n\") if line]\n",
    "    elif file_format == \"xlsx\":\n",
    "        return [response.content]\n",
    "    elif file_format == \"xml\":\n",
    "        return [response.text]\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def get_xml_namespace(element):\n",
    "    m = re.match(r\"\\{(.*)\\}\", element.tag)\n",
    "    return m.groups()[0] if m else \"\"\n",
    "\n",
    "\n",
    "def merge_xml_results(xml_results):\n",
    "    merged_root = ElementTree.fromstring(xml_results[0])\n",
    "    for result in xml_results[1:]:\n",
    "        root = ElementTree.fromstring(result)\n",
    "        for child in root.findall(\"{http://uniprot.org/uniprot}entry\"):\n",
    "            merged_root.insert(-1, child)\n",
    "    ElementTree.register_namespace(\"\", get_xml_namespace(merged_root[0]))\n",
    "    return ElementTree.tostring(merged_root, encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "\n",
    "def print_progress_batches(batch_index, size, total):\n",
    "    n_fetched = min((batch_index + 1) * size, total)\n",
    "    print(f\"Fetched: {n_fetched} / {total}\")\n",
    "\n",
    "\n",
    "def get_id_mapping_results_search(url):\n",
    "    parsed = urlparse(url)\n",
    "    query = parse_qs(parsed.query)\n",
    "    file_format = query[\"format\"][0] if \"format\" in query else \"json\"\n",
    "    if \"size\" in query:\n",
    "        size = int(query[\"size\"][0])\n",
    "    else:\n",
    "        size = 500\n",
    "        query[\"size\"] = size\n",
    "    compressed = (\n",
    "        query[\"compressed\"][0].lower() == \"true\" if \"compressed\" in query else False\n",
    "    )\n",
    "    parsed = parsed._replace(query=urlencode(query, doseq=True))\n",
    "    url = parsed.geturl()\n",
    "    request = session.get(url)\n",
    "    check_response(request)\n",
    "    results = decode_results(request, file_format, compressed)\n",
    "    total = int(request.headers[\"x-total-results\"])\n",
    "    print_progress_batches(0, size, total)\n",
    "    for i, batch in enumerate(get_batch(request, file_format, compressed), 1):\n",
    "        results = combine_batches(results, batch, file_format)\n",
    "        print_progress_batches(i, size, total)\n",
    "    if file_format == \"xml\":\n",
    "        return merge_xml_results(results)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_id_mapping_results_stream(url):\n",
    "    if \"/stream/\" not in url:\n",
    "        url = url.replace(\"/results/\", \"/results/stream/\")\n",
    "    request = session.get(url)\n",
    "    check_response(request)\n",
    "    parsed = urlparse(url)\n",
    "    query = parse_qs(parsed.query)\n",
    "    file_format = query[\"format\"][0] if \"format\" in query else \"json\"\n",
    "    compressed = (\n",
    "        query[\"compressed\"][0].lower() == \"true\" if \"compressed\" in query else False\n",
    "    )\n",
    "    return decode_results(request, file_format, compressed)\n",
    "\n",
    "\n",
    "job_id = submit_id_mapping(\n",
    "    from_db=\"UniProtKB_AC-ID\", to_db=\"UniProtKB\", ids=[\"A0FGR8\", \"O00148\", \"O00410\"]\n",
    ")\n",
    "if check_id_mapping_results_ready(job_id):\n",
    "    link = get_id_mapping_results_link(job_id)\n",
    "    # results = get_id_mapping_results_search(link)\n",
    "    # Equivalently using the stream endpoint which is more demanding\n",
    "    # on the API and so is less stable:\n",
    "    # results = get_id_mapping_results_stream(link)\n",
    "\n",
    "\n",
    "print(link)\n",
    "# {'results': [{'from': 'P05067', 'to': 'CHEMBL2487'}], 'failedIds': ['P12345']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link 통해 나온 결과는 stream.. 말그대로 텍스트의 엄청난 스트림!!\n",
    "# get_id_mapping_results_stream 함수가 이 텍스트를 디코딩해서 필요한 부분만 뽑을 수 있게 해줌\n",
    "# 규칙은 %2C[식별자], tap separated (.tsv) 형식 텍스트임.\n",
    "\n",
    "tsv_rst = get_id_mapping_results_stream(str(link)+'?compressed=false&fields=accession%2Creviewed%2Cid%2Cprotein_name%2Cgene_names%2Corganism_name%2Clength%2Csequence&format=tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>Entry</th>\n",
       "      <th>Reviewed</th>\n",
       "      <th>Entry Name</th>\n",
       "      <th>Protein names</th>\n",
       "      <th>Gene Names</th>\n",
       "      <th>Organism</th>\n",
       "      <th>Length</th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0FGR8</td>\n",
       "      <td>A0FGR8</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>ESYT2_HUMAN</td>\n",
       "      <td>Extended synaptotagmin-2 (E-Syt2) (Chr2Syt)</td>\n",
       "      <td>ESYT2 FAM62B KIAA1228</td>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>921</td>\n",
       "      <td>MTANRDAALSSHRHPGCAQRPRTPTFASSSQRRSAFGFDDGNFPGL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O00148</td>\n",
       "      <td>O00148</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>DX39A_HUMAN</td>\n",
       "      <td>ATP-dependent RNA helicase DDX39A (EC 3.6.4.13...</td>\n",
       "      <td>DDX39A DDX39</td>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>427</td>\n",
       "      <td>MAEQDVENDLLDYDEEEEPQAPQESTPAPPKKDIKGSYVSIHSSGF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O00410</td>\n",
       "      <td>O00410</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>IPO5_HUMAN</td>\n",
       "      <td>Importin-5 (Imp5) (Importin subunit beta-3) (K...</td>\n",
       "      <td>IPO5 KPNB3 RANBP5</td>\n",
       "      <td>Homo sapiens (Human)</td>\n",
       "      <td>1097</td>\n",
       "      <td>MAAAAAEQQQFYLLLGNLLSPDNVVRKQAEETYENIPGQSKITFLL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     From   Entry  Reviewed   Entry Name  \\\n",
       "0  A0FGR8  A0FGR8  reviewed  ESYT2_HUMAN   \n",
       "1  O00148  O00148  reviewed  DX39A_HUMAN   \n",
       "2  O00410  O00410  reviewed   IPO5_HUMAN   \n",
       "\n",
       "                                       Protein names             Gene Names  \\\n",
       "0        Extended synaptotagmin-2 (E-Syt2) (Chr2Syt)  ESYT2 FAM62B KIAA1228   \n",
       "1  ATP-dependent RNA helicase DDX39A (EC 3.6.4.13...           DDX39A DDX39   \n",
       "2  Importin-5 (Imp5) (Importin subunit beta-3) (K...      IPO5 KPNB3 RANBP5   \n",
       "\n",
       "               Organism Length  \\\n",
       "0  Homo sapiens (Human)    921   \n",
       "1  Homo sapiens (Human)    427   \n",
       "2  Homo sapiens (Human)   1097   \n",
       "\n",
       "                                            Sequence  \n",
       "0  MTANRDAALSSHRHPGCAQRPRTPTFASSSQRRSAFGFDDGNFPGL...  \n",
       "1  MAEQDVENDLLDYDEEEEPQAPQESTPAPPKKDIKGSYVSIHSSGF...  \n",
       "2  MAAAAAEQQQFYLLLGNLLSPDNVVRKQAEETYENIPGQSKITFLL...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기서 .tsv 형식을 df으로 바꿀 수 있음.\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def get_data_frame_from_tsv_results(tsv_results):\n",
    "    reader = csv.DictReader(tsv_results, delimiter=\"\\t\", quotechar='\"')\n",
    "    return pd.DataFrame(list(reader))\n",
    "\n",
    "tmp = get_data_frame_from_tsv_results(tsv_rst)\n",
    "\n",
    "tmp.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "596c9d63e29fcbf91eba4fd757e6e698bf89c9e1b516a0889b14eef5ff86a8c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
